[{"uri":"https://sds-aau.github.io/DSBA-2022/m2/01_networks/1_networks/","title":"Basics Network Analysis","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session introduces unsupervised machine learning (UML)\nRecommended Datacamp exercises: Introduction to network Analysis in Python Introduction to network Analysis in R Theory: Network Analysis R Application - Network Analysis Python Application - Network Analysis Video Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides available here Follow along HTML Notebook Colab Notebook Video 1: Intro \u0026 Ecosystem Video 2: Network Measures/h2\u003e Video 3: Mini-Case/h2\u003e Follow along Colab Notebook Video 1: Intro \u0026 Ecosystem Video 2: Network Measures Video 3: Mini-Case "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/02_basics/01_stat_prog/","title":"Basics Statistical Programming","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session is a basic introduction to statistical programming as well as a short brush-up on data more generally. For some, this will be \u0026ldquo;old news\u0026rdquo;, but many will certainly benefit from reviewing this material. We start with a generall theory lecture on data structures and properties, and then dive into R and Python specific applications of statistical programming.\nTheory: Introduction to Data R Application - Statistical Programming Python Application - Statistical Programming Video Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides available here Follow along HTML Notebook Colab Notebook Video Follow along Colab Notebook Video Part 1 Video Part 2 Video Part 3 "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/00_workshops/01_workshop1/","title":"Day 1 - NLP","tags":[],"description":"","content":"Practical info Place: DHØ 1.52 Time: 11:40\nSchedule for the day Session 1: Review Hate-speech classifier Notebook Session 2: Work on NLP case in groups Session 3 - 15:00-16:00: Guest Webinar - AI Implementations in SMEs (in collaboration with AI Denmark and AI 4 The People) Context - Exercise: Presidential Debate 2020 Yes, we are going back in time to the Presidential Debate in the US 2020 - the time of lots of unhappy Tweeting. It\u0026rsquo;s just too good a dataset and case to let it go\u0026hellip;\nData Political tweets: https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pol_tweets.gz from https://github.com/alexlitel/congresstweets We\u0026rsquo;ve preprocessed a bit to make things easier. 1: Dems. 0: Rep.\nTweets around the time of the debate in oktober 20 (8000): https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pres_debate_2020.gz\nBoth datasets are in JSON format. Task: Build a classifier that can distinguish Dem/Rep tweets. Bonus: 1. Explore discussed topics; 2. find out what drives predictions.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/09_workshops/w1/","title":"Day 1 UML","tags":[],"description":"","content":"Practical info Place: Ovnhallen Time: 11.40-17.00\nPower supply in this venue has been known to be sub-optimal. Have a charged computer with you. Perhaps, consider bringing a power-splitter / extender.\nSchedule for the day Time Activity Data Session 1 11.40-12.45 Focus on DataViz mixed Session 2 13.15-14.15 Focus on UML Digital Nomads Session 3 14.30-15.30 Interactive DataViz AirBnb Session 4 15.45-15.45 DataViz Groupwork mixed 16.45-17.00 Wrapping up the day mixed Sessions Session 1 DataViz focus Colab Session 2 comming soon Session 3 comming soon Datasets \u0026amp; Context Digital Nomad Dataset In this workshop we are going to explore the nomad data using unsupervised ML techniques. This time we will focus on the city data, whichn you have not sen so far. You will find the data for today\u0026rsquo;s session here: https://sds-aau.github.io/SDS-master/M1/data/cities.csv\nWe used the data in a research project some years ago and you can check out a conference presentation below:\nAddittional ressources R stuff EDA and UML on NOMAD data "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/10_assignments/exercise1/","title":"Exercise 1 - EDA with Digital Nomads","tags":[],"description":"","content":" This is not an assignment, just a voluntary exercise for you to test if you grasp the concepts in the corresponding topics. It does not have to be handed in anywhere\nData You are given 2 datasets from https://nomadlist.com - A community page for remote workers worldwide. Further, you are provided with a countries list, a dataset containing information on countries, regions and countrycodes.\nTrips data: holds ~46k individual trips of travelers on the platform. https://sds-aau.github.io/SDS-master/M1/data/trips.csv People data: contains some personal information on 4k travelers. https://sds-aau.github.io/SDS-master/M1/data/people.csv Country data: Holds countrycodes, contrynames and region-associations. https://sds-aau.github.io/SDS-master/M1/data/countrylist.csv Tasks Preprocessing a. Trips: transform dates into timestamps (note: in Python, you will have to coerce errors for faulty dates) b. Calculate trip duration in days (you can use loops, list comprehensions or map-lambda-functions (python) to create a column that holds the numerical value of the day. You can also use the datetime package.) c. Filter extreme (fake?) observations for durations as well as dates - start and end (trips that last 234565 days / are in the 17th or 23rd century) The minimum duration of a trip is 1 day! Hint: use percentiles/quantiles to set boundaries for extreme values - between 1 and 97, calculate and store the boundaries before subsetting. R-hint: Use percent_rank(as.numeric(variable)) to create percentiles\nd. Join the countrylist data to the trips data-frame using the countrycode as a key e. [Only for python users ] Set DateTime index as the start date of a trip\nPeople a. How many people have a least a “High School” diploma? Hint: For this calculation remove missing value-rows or fill with “False”. b. How many “Startup Founders” have attained a “Master’s Degree”? Bonus: compared to people who don’t have a formal higher education (e.g. by using the “False” occurrences)? c. Who is the person with a Master’s Degree that has the highest number of followers? Bonus: Explore the individual further, what else can you find out?\nTrips a. Which country received the highest number of trips? – And which the lowest? b. Which region received the highest number of trips in 2017? Use the start of trips as a time reference. c. Which country in “Western Europe” did travelers spent least time? – Provide visualization d. Do nomad Startup Founders tend to have shorter or longer trips on average?\nSolutions R team :::: HERE :::: Py team :::: HERE :::: "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/00_assignments/exercise1/","title":"Exercise 1 - Network Analysis","tags":[],"description":"","content":"Introduction In th first Part 2, you will replicate a well known network analysis, with different data and some twists. Data: The data is to be found at: https://github.com/SDS-AAU/SDS-master/tree/master/00_data/network_krackhard (Hint: You neet to download the raw data) Data: What do I get? Background Let the fun begin. You will analyze network datacollected from the managers of a high-tec company. This dataset, originating from the paper below, is widely used in research on organizational networks. Time to give it a shot as well. Krackhardt D. (1987). Cognitive social structures. Social Networks, 9, 104-134. The company manufactured high-tech equipment on the west coast of the United States and had just over 100 employees with 21 managers. Each manager was asked to whom do you go to for advice and who is your friend, to whom do you report was taken from company documents. Description\nThe dataset includes 4 files - 3xKrack-High-Tec and 1x High-Tec-Attributes. Krack-High-Tec includes the following three 21x3 text matrices:\nADVICE, directed, binary FRIENDSHIP, directed, binary REPORTS_TO, directed, binary Column 1 contains the ID of the ego (from where the edge starts), and column 2 the alter (to which the edge goes). Column 3 indicates the presence (=1) or absence (=0) of an edge.\nHigh-Tec-Attributes includes one 21x4 valued matrix.\nID: Numeric ID of the manager AGE: The managers age (in years) TENURE: The length of service or tenure (in years) LEVEL: The level in the corporate hierarchy (coded 1,2 and 3; 1 = CEO, 2 = Vice President, 3 = manager) DEPT: The department (coded 1,2,3,4 with the CEO in department 0, ie not in a department) Tasks 1. Create a network Generate network objects for the companies organizational structure (reports to), friendship, advice This networks are generated from the corresponding edgelists Also attach node characteristics from the corresponding nodelist 2. Analysis Make a little analysis on:\nA: Network level characteristics. Find the overal network level of:\nDensity Transistivity (Clustering Coefficient) Reciprocity \u0026hellip; for the different networks. Describe and interpret the results. Answer the following questions:\nAre relationships like friendship and advice giving usually reciprocal? Are friends of your friends also your friends? Are the employees generally more likely to be in a friendship or advice-seeking relationship? B: Node level characteristics: Likewise, find out:\nWho is most popular in the networks. Who is the most wanted friend, and advice giver? Are managers in higher hirarchy more popular as friend, and advice giver? C: Relational Characteristics: Answer the following questions:\nAre managers from the same 1. department, or on the same 2. hirarchy, 3. age, or 4. tenuere more likely to become friends or give advice? (hint: assortiativity related) Are friends more likely to give each others advice? 3. Visualization Everything goes. Show us some pretty and informative plots. Choose what to plot, and how, on your own. Interpret the results and share some insights.\nSolutions R team :::: HERE :::: Py team (coming\u0026hellip;I know I\u0026rsquo;m late) "},{"uri":"https://sds-aau.github.io/DSBA-2022/info/","title":"Info, Schedule &amp; Co","tags":[],"description":"","content":"General info about the semester are to be found here, including topics, schedule, and infrastructure. Here, we also point towards additional resources to acquire and apply your data science skills.\nIntro Slides "},{"uri":"https://sds-aau.github.io/DSBA-2022/info/01_infrastructure/","title":"Infrastructure","tags":[],"description":"","content":"Main Infrastructure MS teams : Join our MS our teams channel to get updates from us, Q\u0026amp;A, and talk to your peers. Canvas : We will not use it from now on for content, but for the sake of completeness. However, you will still find your calendar there and we may use it from time to time to send out mass-mails to you all. Datacamp : Get access to all the Datacamp premium content for free to facilitate your data science journey. Use your CBS mail here when signing up. Adittional Infrastructure used Data Science Cloud services Collaborative coding and presentation Github Provides internet hosting for software development and version control using Git. It offers the distributed version control and source code management (SCM) functionality of Git, plus its own features. It is also commonly used to host open-source projects, including data science projects. If you do not have it already, you are advice to create an account to manage and showcase your work during this semester. Cloud-based computational notebook: Google Colab: Googles popular service for editing, running \u0026amp; sharing Jupyter notebooks (Only Python Kernel, but R kernel can be accessed via some tricks). Has a pro version for some 10$ in case you need further ressources. You can also easily mount your google drive to directly acess your data. Deepnote: New popular online notebook service with good integration to other services (Python, R \u0026amp; more) Kaggle: Also provides their own cloud-based service co create and run computational notebooks. Convenient, unlimited, but a bit slow (Pyhton, R ). Cloud-Computing UCLOUD: DK university collaboration for HPC. Apparently, CBS is now also on board (untested) Getting started Jupyter/Colab Intro Python Colab Github Hello World The Markdown Guide: Introduction to markdown, the formating language used in ipython notebooks. "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/02_nlp/0_nlp_theory/","title":"Natural Language Processing - Theory","tags":[],"description":"","content":"In this part you\u0026rsquo;ll get a 100% code-free intro to (statistical) Natural Language Processing\nRecommended Datacamp exercises: Python Intro to NLP Python - also good Feature Engineering for NLP in Python R Introduction to Natural Language Processing in R NLP intro - level of analysis Text representation From BoW to Topic Modeling and Embeddings (optional) History of NLP in Industry - Yoav Goldberg "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/01_networks/","title":"Network Analysis","tags":[],"description":"","content":"This chapter introduces you to network analysis and working with relational data.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/03_ml/01_uml/","title":"Unsupervised Machine Learning (UML)","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session introduces unsupervised machine learning (UML)\nRecommended Datacamp exercises: Python R Theory: Unsupervised ML R Application - UML Python Application - UML Video Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides available here Follow along HTML Notebook Colab Notebook Video 1: EDA \u0026 Dimensionality Reduction Video 2: Clustering Follow along Colab Notebook Video 1: EDA \u0026 Dimensionality Reduction Video 2: EDA \u0026 Clustering "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/01_warmup/","title":"Warmup (W36)","tags":[],"description":"","content":"This warmup introduces on a high level to the use of DS/ML methods in Business. The second part of the warmup provides an overview of the infrastructure used at DSBA.\nData Science in Business In this chapter you learn about data science and ML applications in business applications. The presentations have been recorded as part of a workshop on Data Science for Business Leaders. Contents are not part of the course curriculum, yet a good way to get more familiar with terminology.\nIntroduction to the Data Science Ecosystem Overview of software, platforms and most other things you are going to work with at DSBA. You only have to work in one of the 2 languages but it\u0026rsquo;s still nice to know how the other ecosystem looks like\u0026hellip;\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/02_nlp/1_nlp_string/","title":"Application: String Manipulation","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nR Application - String Manipulation Python Application - String Manipulation Follow along HTML Notebook Video Follow along Colab Notebook Video 1 Video 2 "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/","title":"Applied Data Science and Machine Learning","tags":[],"description":"","content":"M1 - Applied Data Science and Machine Learning This module provides a condensed introduction to the “Data Science Pipeline”, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nContent by week for this module Click on the to do for the week to see what you should do to keep up with the module.\nW 36: Data Manipulation, Exploratory Data Analysis (EDA), Data Visualization Weekly to do Course Material (Watch videos, study/run notebooks, solve provided exercises, optimally study suggested further material) M1: Warmup (not mandatory) M1: Basics M1: Data Manipulation M1: Data Visualizatioh Introduction to R \u0026amp; Python (Datacamp, at least one necessary if no prior experience) Intro to Python and/or [Intro to R Statistics Refresher (Datacamp, recommended if no prior statistics classes, choose either R or Python) General Intyroduction to statistics (no coding) Python Statistical Thinking 1 Statistical Thinking 2 Intro to linear modelling R: Introduction to data in R Foundation of probability; Correlation and regression W 37: Unsupervised Machine Learning (UML), Supervised Machine Learning (SML) Weekly to do Course Material (Watch videos, study/run notebooks, solve provided exercises, optimally study suggested further material) M1: Introduction to Unsupervised Machine Learning. M1: Introduction to supervised Machine Learning **Continue with Datacamp) **recommended but not mandatory) Python UML Intro to supervised learning Decision Tree modeling R UML Supervised classification Supervised Regression Prepare for Q\u0026amp;A (discuss in teams, send/prepare questions) W 38: Workshop \u0026amp; project work\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/02_basics/02_data_manipulation/","title":"Basics Data Manipulation","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session introduces some fundamental concepts of data manipulation and Exploratory Data Analysis. We will again start with a theoretical lecture on fundamental concepts and techniques in data manipulation, and afterwards again explore them in R and Python specific applications. These applications are complemented by recap exercises to test yourself.\nTheory: Fundamentals of Data Manipulation R Application - Data Manipulation Python Application - Data Manipulation Video Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides available here Follow along HTML Notebook Colab Notebook Complementary Exercises 1 Basics 2 Joins 3 Challange Video Follow along Colab Notebook Video "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/02_basics/","title":"Basics of Statistical Programming and Data Manipulation (W36)","tags":[],"description":"","content":"This chapter is a basic introduction to statistical programming as well as a short brush-up on data more generally. For some, this will be \u0026ldquo;old news\u0026rdquo;, but many will certainly benefit from reviewing this material. Afterwards, we introduces some fundamental concepts of data manipulation and exploratory data analysis (EDA).\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/09_workshops/w2/","title":"Day 2 SML","tags":[],"description":"","content":"Practical info Place: SP213 Time: 08.00-13.20\nSchedule for the day Time Activity Data Session 1 08:00-09:00 SML - Regression AirBnb Session 2 09:10-10:15 SML - Classification HR data - Updated Description Session 3 10:25-11:30 SML - Explainability - Session 2 09:10-10:15 SML - Classification HR data - Updated Description Session 3 10:25-11:30 SML - Explainability - - 12:00-13:00 Groupwork Dataviz - - 13:00-13:20 Wrap up, Assignment handout - Sessions Airbnb: regression form scratch\nDatasets \u0026amp; Context \u0026hellip;\nFurther ressources "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/10_assignments/exercise2/","title":"Exercise 2 - UML with Pokemon","tags":[],"description":"","content":" This is not an assignment, just a voluntary exercise for you to test if you grasp the concepts in the corresponding topics. It does not have to be handed in anywhere\nDescription This time you will work with Pokemon data. No data munging needed. Just old-school (U)ML.\nData The data is available through the URL: https://sds-aau.github.io/SDS-master/00_data/pokemon.csv. It contains data on 800 Pokemon from the 1st to the 6th generation.\nTasks Give a brief overview of data, what variables are there, how are the variables scaled and variation of the data columns. Execute a PCA analysis on all numerical variables in the dataset. Hint: Don\u0026rsquo;t forget to scale them first. Use 4 components. What is the cumulative explained variance ratio? Hint: I am not sure this terminology and code was introduced during class, but try and look into cumulative explained variance and sklearn(package) and see if you can figure out the code needed. Use a different dimensionality reduction method (eg. UMAP/NMF) – do the findings differ? Perform a cluster analysis (KMeans) on all numerical variables (scaled \u0026amp; before PCA). Pick a realistic number of clusters (up to you where the large clusters remain mostly stable). Visualize the first 2 principal components and color the datapoints by cluster. Inspect the distribution of the variable Type1 across clusters. Does the algorithm separate the different types of pokemon? Perform a cluster analysis on all numerical variables scaled and AFTER dimensionality reduction and visualize the first 2 principal components. Again, inspect the distribution of the variable “Type 1” across clusters, does it differ from the distribution before dimensionality reduction? Solutions R team :::: HERE :::: Py team :::: HERE :::: - Includes also some SML "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/00_assignments/exercise2/","title":"Exercise t 2 - NLP","tags":[],"description":"","content":"Introduction: Building a Hate Speech Classifier This assignment is less structured than previous individual assignments.\nYou are given a collection of approximately 25k tweets that have been manually (human) annotated. class denotes: 0 - hate speech, 1 - offensive language, 2 - neither\nhttps://github.com/SDS-AAU/SDS-master/raw/master/M2/data/twitter_hate.zip\n1. Preprocessing and vectorizaion. Justify your choices and explain possible alternatives (e.g. removing stopwords, identifying bi/tri-grams, removing verbs or use of stemming, lemmatization etc.)\nCreate a bag-of-words representation, apply TF-IDF and dimensionality reduction (LSA-topic modelling alternatively simply PCA or SVD) to transform your corpus into a feature matrix. 2. Explore and compare the 2 \u0026ldquo;classes of interest\u0026rdquo; - hate speech vs offensive language. Can you see differences by using simple count-based approaches? Can you identify themes (aka clusters / topics) that are specific for one class or another? Explore them using, e.g. simple crosstabs - topic vs. class and to get more detailed insights within-cluster top (TF-IDF) terms. (This step requires preprocessed/tokenized inputs). 3. Build an ML model that can predict hate speech Use the ML pipeline (learned in M1) to build a classification model that can identify offensive language and hate speech. It is not an easy task to get good results. Experiment with different models on the two types of text-representations that you create in 2.\nBonus: Explore missclassified hate speech tweets vs those correctly predicted. Can you find specific patterns? Can you observe some topics that are more prevalent in those that the model identifies correcly?\nThe best-reported results for this dataset are.\nClass Precision 0 0.61 1 0.91 2 0.95 Overall 0.91 Here advanced NLP feature engineering has been used, and thus everything around an overall accuracy of 85 is fine. You will see that it is not easy to lift class 0 accuracy over 0.5\nGood Luck!\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/01_networks/2_networks_intermediate/","title":"Intermediate Network Analysis","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session introduces to slightly more advanced concepts in network analysis\nR Application - Intermediate Network Analysis Python Application - Intermediate Network Analysis Follow along HTML Notebook Notebook Colab Notebook Video 1 Follow along Colab Notebook Video 1: Intro Video 2: Case "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/01_warmup/01_business/","title":"Intro Talk - Data Science in business","tags":[],"description":"","content":" Use the tabs to access content\nThis Video series contains a 3-part talk by Daniel and Roman on data science applications in business\nFollow along:\nSlides DS in business 1 - Current state 2 - Intro ML 3 - DS Teams Data Science in business 1 - Current state Data Science in business 2 - Introduction to Machine Learning Data Science in business 3 -Team and Roles "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/02_nlp/","title":"Natural Language Processing","tags":[],"description":"","content":"This chapter introduces you to Natural Language Processing and how you can work with text data in machine learning pipelines.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/","title":"Network Analysis &amp; NLP","tags":[],"description":"","content":"M2 - Network Analysis \u0026amp; NLP This module provides a condensed introduction analysing two popular forms of unstructured data, namely relational and text data.\nContent by week for this module Click on the to do for the week to see what you should do to keep up with the module\nW 40: Introduction to Network Analysis W 41: Introduction to Natural-Language-Processing (NLP) W 41: Advanced applications in Network and Text Analysis W 42: Assignment work "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/03_ml/02_sml/","title":"Supervised Machine Learning (SML)","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session introduces unsupervised machine learning (UML)\nRecommended Datacamp exercises: Python - Intro to supervised learning Python - Decision Tree modeling R - Supervised classification R - Supervised Regression Theory: Supervised ML R Application - SML Python Application - SML Video 1: Introduction \u0026 Statistics Refresher Video 2: Generalization, Model Classes \u0026 Tuning Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides available here Follow along HTML Notebook Colab Notebook Video 1: Introduction \u0026 ML workflows with tidymodels Video 2: Regression problem case Video 3: Classification problem case Follow along Colab Notebook Video 1: Introduction \u0026 ML/Model fitting Mechanics Video 2: Regression problem case Video 3: Classification problem case and more "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/02_nlp/2_nlp_longtext/","title":"Application: Working with long text","tags":[],"description":"","content":"Working with long text and extracting text elements Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nR Application - Working with long text Python Application - Working with long text Follow along HTML Notebook Preprocessing \u0026 Sentiments Follow along Colab Notebook can be watched at 1.5x 😉 Link to News-scraping tutorial in the notebook. "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/02_basics/03_data_visualization/","title":"Basics Data Visualization","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session introduces some fundamental concepts of data visualization. After a theoretical lecture on types and dimensions of data visualization, we will explore the visualization of different data types, structure, and properties in R and Python specific applications.\nTheory: Elements of Data Visualization R Application - Data Visualization Python Application - Data Visualization Video Slides Use arrows keys on keyboardto navigate. Alternatively fullscreen slides available here Follow along HTML Notebook Colab Notebook Video Follow along Colab Notebook Video Part 1 Video Part 2 "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/10_assignments/group_assignment/","title":"M1 (Mandatory) Assignment","tags":[],"description":"","content":"DSBA 2022 - M1: (Mandatory) Assignment Introduction With the individual assignments, you already performed most of the steps in a typical machine learning pipeline. You imported some data, cleaned it, explored the variables and their relationships using summary statistics and visualisations. You also exercised some standard machine learning preprocessing procedures such as feature scaling and dealing with missing values.\nYou practised unsupervised machine learning techniques for dimensionality reduction (e.g. PCA) and clustering (e.g. KNN) to discover latent relationships between features and groupings of observations. In the final workshop and online material you finally used supervised machine learning for regression and classification problems, where you created models to predict an outcome of interest given some input features.\nNow it is time to bring most these steps together and apply them to a setting that you find interesting. This should apply the following tasks.\nIdentify an interesting problem that can be tackled using data science techniques. Select and obtain relevant data to do so. Clean and manipulate the data to make it useful for data science techniques. Carry out an exploratory data analysis to provide intuition into the content of the data, and interesting relationships to be found in it. You might unsupervised ML techniques to discover latent relationships within the data. Use supervised ML techniques to create models that predict an outcome of interest. Document your workflow in a reconstructable manner. Report your findings in an accessible manner. Task description Data \u0026amp; Problem identification In this exercise, you are asked to choose and obtain a dataset you consider interesting and appropriate for the tasks required. Some of you may already have some ideas about interesting datasets. There are many open datasets available on the internet (e.g. kaggle or individual projects like Stanford Open Policing or download some of the Datacamp project datasets) here a recent list of open data repositories for inspiration\nIf you instead want to collect your data (e.g. scraping Twitter or other platforms) – we will not hold you back. However, consider the timeframe.\nThe data should fulfill the following minimum requirements:\nIt should be big enough to be useful for applying data science techniques (rule of thumb: minimum \u0026gt; 500 observations, \u0026gt; 10 features). It contains an interesting outcome to be predicted via supervised ML. It is not completely trivial and clean, and at least requires a minimum amount of cleaning, munging, preprocessing (eg. no toy dataset such as Iris, diamonds, or cars). Analysis pipeline The analysis to be carried out by you has to contain elements of data manipulation, exploration and supervised ML (unsupervised is optional).\nGenerally, you can combine parts from the individual assignments and use them as a template for the module assignment. Going beyond that is not required (but for sure appreciated). Below a (rather detailed) checklist to make sure you have all the pieces.\nDefinition of a problem statement and a short outline of the implementation Description of data acquisition / how it was collected (by you or the publisher of the data) Data preparation Data cleaning (if needed) Recoding (label encoding, dummy creation etc.) Merging and wrangling (if needed) Exploratory data analysis Relevant! summary statistics Relevant! visualisations Appropriate description (This is important!) Optional! unsupervised machine learning for EDA or SML preprocessing (eg. dimensionality reduction, clustering) Feature scaling (if applicable) Missing data handing (dropping or inputing) Supervised ML Train- / Testset preparation classification or/and regression problem Use of different algorithms (min. 3) compared and ranked according to their performance using appropriate metrics (k-fold cross-validation) you may include hyperparameter tuning (grid-search, adaptive resampling etc.) performance evaluation on the test set (scores, performance reports but also visuals where useful) Many of the steps are optional. So choose which methods you deem helpful and relevant to explore your chosen problem.\nNote: Quality \u0026gt; Quantity. Consider which analysis, summarization, and visualization adds value. Excessive and unselective outputs (e.g. running 20 different models without providing a reason for, providing all possibilities of different plots without discussing and evaluating the insights gained from it) will not be considered helpful but rather distracting.\nDocumentation and Deliverables You are asked to hand in a well commented functional computational notebook\nComputational Notebook The notebook targets a machine-learning literate audience. Here you can go deeper into the technical details and method considerations. Provide thorough documentation of the whole process, the used methods. Describe the intuition behind the selected and used methods, justify choices made, and interpret results (e.g. Why scaling? Why splitting the data? Why certain tabulations and visualizations? What can be seen from \u0026hellip; ?, How did you select a particular algorithm? Why did you scale features in one way or another?).\nFinally Submission: 28.09, 23:59 Hand in on :::\u0026amp;gt; peergrade here \u0026amp;lt;::: "},{"uri":"https://sds-aau.github.io/DSBA-2022/info/02_modules/","title":"Modules","tags":[],"description":"","content":"The course is structured in 3 modules.\nM1: Applied Data Science and Machine Learning This module will prove a condensed introduction to the “Data Science Pipeline”, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nM2: Network Analysis and Natural Language Processing Focuses on analyzing a variety of unstructured data sources. Particularly, students will learn how to explore, analyze, and visualize natural language (text) as well as relational (network) data.\nM3: Deep Learning and Artificial Intelligence for Analytics Introduces to the most recent developments in machine learning, which are deep learning and artificial intelligence applications. The module will provide a solid foundation for this exciting and rapidly developing field. Students will learn whether and how to apply deep learning techniques for business analytics, and acquire proficiency in new methods autonomously.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/01_networks/3_networks_2mode/","title":"Network 2-Mode","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nThis session introduces to multimodel natwork analysis concepts\nR Application -2-mode Network Analysis Python Application - -2-mode Network Analysis Follow along HTML Notebook Notebook Colab Notebook Video 1: Intro Video 2: Application Follow along Colab Notebook 1/a\u003e Colab Notebook 2 Video 1: Intro Video 2: Case "},{"uri":"https://sds-aau.github.io/DSBA-2022/info/03_schedule/","title":"Semester Schedule","tags":[],"description":"","content":"M1: Week 36-38 Topics W 36: Data Manipulation, Exploratory Data Analysis (EDA), Data Visualization W 37: Unsupervised Machine Learning (UML), Supervised Machine Learning (SML) W 38: Workshop \u0026amp; project work M2: Week 39-41 Topics W 39: Introduction to Network Analysis W 40: Introduction to Natural-Language-Processing (NLP) W 41: Workshop \u0026amp; project work M3: Week 43-46 Topics W 43: Introduction to Artificial Neural Networks (ANN) \u0026amp; Deep Learning (DL) W 44: Neural networks for spatial data: Recurrent Neural Networks (RNN) W 45: Neural Networks for sequential data: Recurrent Neural networks (RNN \u0026amp; LSTM) W 46: Workshop \u0026amp; project work Key Dates In-person workshops on CBS campus (mostly Thursday + Friday)\n1: W38: Machine learning case studies 2: W41: Advanced applications in Network and Text Analysis 3: W46: Advanced applications \u0026amp; outlook in deep learning Individual assignment (2 out of 3 need to be passed):\n1: 23.-28.09.2022, 23:59:00 at the latest (Peergrade) 2: 14.-26.10.2022, 23:59:00 at the latest (Peergrade) 3: 14.-16.11.2022, 23:59:00 at the latest (Peergrade) Final exam (individual exam of group project)\nHand-out: 18.11.2022 Hand-in: 01.12.2021 Final exam: 16.12.2021 "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/01_warmup/02_ecosystem/","title":"The Data Science Ecosystem","tags":[],"description":"","content":" Use the tabs to access content\nThis Video series contains an introduction to the data science ecosystem, includng a general as well as a R and Python specific overview.\nR Ecosystem Python Ecosystem DS Ecosystem general Video Slides Video Slides Video Slides "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/03_ml/03-uml-workshop/","title":"Unsupervised Machine Learning - Applied","tags":[],"description":"","content":"This tutorial will provide a hands-on intro to core techniques of unsupervised learning. We will cover, data preprocessing, PCA, NMF, UMAP, K-means and methods of interpreting results of their application.\nNotebook(s) Hands-on Intro to Dimensionality reduction and Clustering Recommended Datacamp exercises: Python Recommended Readings and resources Python Data Science Handbook Chapter 5\nWhat Is Machine Learning? Introducing Scikit-Learn Feature Engineering In Depth: Principal Component Analysis In Depth: k-Means Clustering Implementation tutorials on YT PCA and K-means from this list\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/02_nlp/3_nlp_topic/","title":"Application: Vectorization and Topic Modelling","tags":[],"description":"","content":" Use the tabs to access content. Theory part is general, R \u0026amp; Python application part language specific.\nR Application Python Application Follow along HTML Notebook Follow along Colab Notebook Video Ecosystem Video Tutorial "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/03_ml/","title":"Basics Machine Learning (W37-38)","tags":[],"description":"","content":"This chapter introduces you to the intuition behind unsupervised (UML) and supervised machine (SML) learning, and demonstrates common techniques and workflows. It contrasts UML (=trained on labeled data) with SML (trained on labeled data), and also distinguishes it from traditional inferential statistics (e.g. econometrics).\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/info/04_litetrature/","title":"Literature &amp; Resources","tags":[],"description":"","content":"While this course does not come with a list of mandatory readings, we will often refer to some central resources in python and R, which for the most part can always be accessed in a free and updated online version. We generally recommend you to use these amazing resources for problem-solving and further self-study on the topic.\nMain Literature These pieces of work can be seen as main references for data science using R and Python. We will frequently refer to selected chapters for further study.\nPython VanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O\u0026rsquo;Reilly Media, Inc. Online available here R Wickham, H., \u0026amp; Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data. O\u0026rsquo;Reilly Media, Inc. Online available here Baumer, B., Kaplan, D. \u0026amp; Horton, N. (2020) Modern Data Science with R (2nd Ed.). CRC Press Online available here Kuhn, M., Silge, J. (2020) Tidy Modeling with R Online available here Supplementary literature R R Markdown: The Definitive Guide Efficient R Programming Functional Programming in R (Stanford, Hadley) Exploring Enterprise Databases with R Further Ressources Community Kaggle: Crowdsourced data science challanges. Nowadays also provides a vivid community where you find datasets, notebooks for all kind of data science exercises. Stackoverflow: Q\u0026amp;A community for coding issues. Most coding questions you could come up with have already been answered, or will be answered fast (if you ask right ;)). Danish Data Science Community: Community of Data Scientist here in DK. Go here for finding project partners, check whats thrending in Danish DS, and ask conceptual questions. Tools \u0026amp; Helpers rmd to ipynb converter ipynb to pdf converter "},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/02_basics/04-interactive-dataviz/","title":"Rapid Prototyping with Streamlit","tags":[],"description":"","content":"\nStreamlit was first released in October 2019 and has gained enormous popularity in the past year. The reason behind the framework\u0026rsquo;s success is the ease with which it allows data scientists to build data-driven web apps without the need to deal with frontend development or other dev-ops stuff while allowing them to incorporate all kinds of functions going far beyond just dashboards. Going from a Jupyter Notebook to a Streamlit app just requires adding a few lines of code and rewriting a few minor things.\nFrom ipynb to web-app In this tutorial, we will be going back to this Airbnb EDA Notebook from last week and building a little web-app from it.\nPlan of attack\nWe will isolate the data-prep and visualization parts we are interested in Build \u0026amp; test the app in uCloud Deploy the app via GitHub to the Streamlit Cloud (this step is optional, as you can also deploy via uCloud - ~2kr/day server costs) uCloud is a uninversity cloud service. You get 1000 DKK and 50GB storage to start with but you can apply for more. This is usually granted, as the service is not used a lot. It\u0026rsquo;s a great place to learn about modern platforms, infrastructure and more. You can play with different types of installations in a safe environment. You can also request very powerful machines.\nUCloud Set-up For this project you will need 2 app-containers running: Coder-python and Streamlit. Both can run with minimal CPU/RAM requirements. Streamlit can only run once you created a project in Code-python and saved an app.py file. It is a good idea, to create a public link and connect it to the streamlit-app. Thus, you can try out your app on your phone or share it. Saving changes in app.py will trigger imediate recomplies and your app will update everywhere.\nStreamlit syntax and layout Now, what do we need to turn our notebook into a web app?\n# 1. page-config st.set_page_config(page_title=\u0026#39;Streamlit - Dashboard 🤯\u0026#39;, page_icon=\u0026#34;🚀\u0026#34;, layout=\u0026#39;wide\u0026#39; ) # 2. Page layout - e.g. a title st.title(\u0026#34;AirBnb rentals in Copenhagen 🇩🇰\u0026#34;) Streamlit layout follows your script - things that come first, will be displayed first\u0026hellip;etc.\nLoading and preprocessing the data We can just proceed as in a notebook, but it is useful to rewrite the data loading and preprocessing into a function and add the @st.experimental_singleton decorator. Streamlit performs a re-run every time something is chaged (UI) by the user e.g. a new filter is set. To reduce processing time it\u0026rsquo;s a good idea not to re-run data-loading every single time.\n# LOAD DATA ONLY ONCE @st.experimental_singleton def load_data(): data = pd.read_csv(\u0026#39;http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2022-06-24/visualisations/listings.csv\u0026#39;) # also preprocess as we did in the notebook data = data[data.number_of_reviews \u0026gt; 0] data = data[data.room_type.isin([\u0026#39;Private room\u0026#39;, \u0026#39;Entire home/apt\u0026#39;])] data[\u0026#39;price_z\u0026#39;] = (data[\u0026#39;price\u0026#39;] - data[\u0026#39;price\u0026#39;].mean())/data[\u0026#39;price\u0026#39;].std(ddof=0) data[\u0026#39;price_z\u0026#39;] = data[\u0026#39;price_z\u0026#39;].abs() data = data[data.price_z \u0026lt; 3] data[\u0026#39;log_price\u0026#39;] = np.log(data[\u0026#39;price\u0026#39;]) return data # LOAD THE DATA NOW! data = load_data() The plots to be rendered We will go for 2 plots. A geo-visualization using pydeck and a simple altair bar plot to show prices in different areas of town.\nGeoplot\nlayer = pdk.Layer( \u0026#34;ScatterplotLayer\u0026#34;, data=data[[\u0026#39;name\u0026#39;,\u0026#39;room_type\u0026#39;,\u0026#39;price\u0026#39;, \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;]].dropna(), pickable=True, opacity=0.7, stroked=True, filled=True, radius_scale=10, radius_min_pixels=1, radius_max_pixels=100, line_width_min_pixels=1, get_position=[\u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;], get_radius=10*\u0026#34;log_price\u0026#34;, get_color=[255, 140, 0], get_line_color=[0, 0, 0], ) # Set the viewport location view_state = pdk.ViewState(latitude=data[\u0026#39;latitude\u0026#39;].mean(), longitude=data[\u0026#39;longitude\u0026#39;].mean(), zoom=12, pitch=50) # Renders r = pdk.Deck(layers=[layer], initial_view_state=view_state, #map_style=\u0026#39;mapbox://styles/mapbox/light-v9\u0026#39;, tooltip={\u0026#34;text\u0026#34;: \u0026#34;{name}\\n{room_type}\\n{price}\u0026#34;} ) Altair barplot\nWhen using altair, we need to add one more thing, which is the number of processed observations. Altair doesn\u0026rsquo;t want to process beyond 5000 observations. That means that you have two options. Either you limit your data, as we will do here, or you pre-computed things in e.g. pandas. Here we will use 2 simple if-statements to make sure that data is always max 5000 observations.\nif len(data) \u0026gt; 5000: data_alt = data.sample(5000) if len(data) \u0026lt;= 5000: data_alt = data We use altair to create a bar chart and let it calculate the mean of the price variable (x-axis) with y and colours being split by the room type. We spread the chart across rows that represent neighbourhoods. Also, we add a tooltip that displays the values for the individual bars. We set strokeWidth to 0 to make things a bit tidier.\nprice_chart = alt.Chart(data).mark_bar().encode( x=\u0026#39;mean(price):Q\u0026#39;, y=alt.Y(\u0026#39;room_type:O\u0026#39;,axis=alt.Axis(labels=False), title=\u0026#34; \u0026#34;), color=alt.Color(\u0026#39;room_type:N\u0026#39;, scale=alt.Scale(scheme=\u0026#39;lightorange\u0026#39;)), row=\u0026#39;neighbourhood:N\u0026#39;, tooltip=[\u0026#34;neighbourhood:N\u0026#34;, \u0026#34;mean(price):Q\u0026#34;] ).configure_view(strokeWidth=0).interactive() Introductin UI / Filters We\u0026rsquo;ll introduce 2 filters in the main page (you could also move them to the sidebar): price range and neighbourhood. As you can see below, the st.sliderand st.multiselect produce python objects (tuple and list) that we can use to filter our data DataFrame with plain Pandas. e.g. When only 3 neighbourhoods are selected neighbourhood_select will turn a list of 3 elements and data[data.neighbourhood.isin(neighbourhood_select)] will result in a dataframe where only listings from these areas are present.\n#filter for price-range price_selected = st.slider(\u0026#34;Select price range\u0026#34;, min_value = int(data.price.min()), max_value= int(data.price.max()), value = (300,3000), step=50) data = data[(data.price \u0026gt; price_selected[0]) \u0026amp; (data.price \u0026lt; price_selected[1])] #filter for neighborhoods neighbourhood_select = st.multiselect(\u0026#39;Select neighbourhoods\u0026#39;, data.neighbourhood.unique(), data.neighbourhood.unique()) data = data[data.neighbourhood.isin(neighbourhood_select)] Rendering the visualizations So far nothing is displayed. We only created the objects r(pydeck map) and price_chart altair chart. It\u0026rsquo;s up to you how you like to handle it. I find it a bit easier to separate compute and render parts. Here we are going to add a horizontal column split (to make things a bit more pretty). We split the screen into 5 parts, where the first gets 3 and the second 2. To display our chats, we use the streamlit functions st.pydeck_chart and st.altair_chart. You will find many more options in the streamlit documentation.\nrow1_1, row1_2 = st.columns((3, 2)) with row1_1: st.pydeck_chart(r) with row1_2: st.altair_chart(price_chart, use_container_width=False) Requirements Our imports for that project look like this:\nimport streamlit as st import streamlit.components.v1 as components import pydeck as pdk import numpy as np import pandas as pd import altair as alt alt.renderers.set_embed_options(theme=\u0026#39;dark\u0026#39;) In contrast to a notebook, we cannot really install packages on the fly. Considering a simple python deployment environment, we need to specify what packages (aside from streamlit) need to be installed so that the app can run. This is often done by adding a requirements.txt file to the project folder. We add all libraries that we loaded just to be sure. This is not always the best idea, as things can clash\u0026hellip; but that is a chapter of its own to be covered at a later point. 😵\npydeck altair numpy pandas All code in one place requirements.txt Expand to see code... pydeck numpy pandas altair app.py Expand to see code... #imports import streamlit as st import streamlit.components.v1 as components import pydeck as pdk import numpy as np import pandas as pd import altair as alt alt.renderers.set_embed_options(theme=\u0026#39;dark\u0026#39;) # page config st.set_page_config(page_title=\u0026#39;Streamlit - Dashboard 🤯\u0026#39;, page_icon=\u0026#34;🚀\u0026#34;, layout=\u0026#39;wide\u0026#39; ) #load data @st.experimental_singleton def load_data(): data = pd.read_csv(\u0026#39;http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2022-06-24/visualisations/listings.csv\u0026#39;) data = data[data.number_of_reviews \u0026gt; 0] data = data[data.room_type.isin([\u0026#39;Private room\u0026#39;, \u0026#39;Entire home/apt\u0026#39;])] data[\u0026#39;price_z\u0026#39;] = (data[\u0026#39;price\u0026#39;] -data[\u0026#39;price\u0026#39;].mean())/data[\u0026#39;price\u0026#39;].std(ddof=0) data[\u0026#39;price_z\u0026#39;] = data[\u0026#39;price_z\u0026#39;].abs() data = data[data.price_z \u0026lt; 3] return data data = load_data() # 2. Page layout - e.g. a title st.title(\u0026#34;AirBnb rentals in Copenhagen 🇩🇰\u0026#34;) #filter for price-range price_selected = st.slider(\u0026#34;Select price range\u0026#34;, min_value = int(data.price.min()), max_value= int(data.price.max()), value = (300,3000), step=50) data = data[(data.price \u0026gt; price_selected[0]) \u0026amp; (data.price \u0026lt; price_selected[1])] #filter for neighborhoods neighbourhood_select = st.multiselect(\u0026#39;Select neighbourhoods\u0026#39;, data.neighbourhood.unique(), data.neighbourhood.unique()) data = data[data.neighbourhood.isin(neighbourhood_select)] #geoplot layer = pdk.Layer( \u0026#34;ScatterplotLayer\u0026#34;, data=data[[\u0026#39;name\u0026#39;,\u0026#39;room_type\u0026#39;,\u0026#39;price\u0026#39;, \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;]].dropna(), pickable=True, opacity=0.7, stroked=True, filled=True, radius_scale=10, radius_min_pixels=1, radius_max_pixels=100, line_width_min_pixels=1, get_position=[\u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;], get_radius=10*\u0026#34;log_price\u0026#34;, get_color=[255, 140, 0], get_line_color=[0, 0, 0], ) # Set the viewport location view_state = pdk.ViewState(latitude=data[\u0026#39;latitude\u0026#39;].mean(), longitude=data[\u0026#39;longitude\u0026#39;].mean(), zoom=12, pitch=50) # Renders r = pdk.Deck(layers=[layer], initial_view_state=view_state, #map_style=\u0026#39;mapbox://styles/mapbox/light-v9\u0026#39;, tooltip={\u0026#34;text\u0026#34;: \u0026#34;{name}\\n{room_type}\\n{price}\u0026#34;} ) # prefilter for altair if len(data) \u0026gt; 5000: data_alt = data.sample(5000) if len(data) \u0026lt;= 5000: data_alt = data #altair plot price_chart = alt.Chart(data).mark_bar().encode( x=\u0026#39;mean(price):Q\u0026#39;, y=alt.Y(\u0026#39;room_type:O\u0026#39;,axis=alt.Axis(labels=False), title=\u0026#34; \u0026#34;), color=alt.Color(\u0026#39;room_type:N\u0026#39;, scale=alt.Scale(scheme=\u0026#39;lightorange\u0026#39;)), row=\u0026#39;neighbourhood:N\u0026#39;, tooltip=[\u0026#34;neighbourhood:N\u0026#34;, \u0026#34;mean(price):Q\u0026#34;] ).configure_view(strokeWidth=0).interactive() # display and layout row1_1, row1_2 = st.columns((3, 2)) with row1_1: st.pydeck_chart(r) with row1_2: st.altair_chart(price_chart, use_container_width=False) "},{"uri":"https://sds-aau.github.io/DSBA-2022/info/05_requirements_project/","title":"Semester Project Requirements","tags":[],"description":"","content":"Format Project report (30-ish pages - max. 45\u0026hellip; depending on group size) Some study relation (but that is debatable and not necessarily required) Report is a (semi/non) technical documentation. Think about a corporate censor that you try to inform academia. Less (but some) theory, more application. Also provide: Functional and self-contained notebook Happy to see GitHub repos (which you can use as your portfolio in the job market) Content Problem formulation with some practical and theoretical motivation (no huge literature discussion) Methodology (not a critical realist vs positivist discussion but some ideas about what can be concluded potentially) Data sourcing and pre-processing strategy Overall architecture of the model(s) Modelling (incl. finetuning) Results Discussion / Conclusion Scope Uses different methods from the course (at least 2 modules) in a creative way Downloading data from kaggle/github and running an ML model is probably not enough for a good performance Creative combinations of methodologies, please: combine financial data with social media data to look at equity development extract information from text data and create networks. Use network indicators to supplement company data Evaluation will focus on correct application and communication of DS methods The level of \u0026ldquo;technicality\u0026rdquo; is as in the course with emphasis on application and intuition, not on ML engineering / mathematics However, you will need to demonstrate insight into statistics on a level that is required to discuss your assignment e.g. interpret and discuss performance indicators, outline strategies for improvement e.g. under/oversampling. "},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/00_assignments/","title":"Assignments","tags":[],"description":"","content":"Here you will find all the assignments for this module.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/09_workshops/","title":"Workshops","tags":[],"description":"","content":"Our first round of live workshops. For details, see the individual days.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m1/10_assignments/","title":"Assignments &amp; Exercises","tags":[],"description":"","content":"Here you will find all the assignments for this module.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/m2/00_workshops/","title":"Workshops","tags":[],"description":"","content":"In the live workshops we will aim at working with real world data in groups in class. Towards the end of the session, we will be collecting and disckussin results.\nDay 1 NLP Classification and exploration of text corpus. Training and application to new data. "},{"uri":"https://sds-aau.github.io/DSBA-2022/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://sds-aau.github.io/DSBA-2022/","title":"Data Science for Business Applications 2022","tags":[],"description":"","content":"Data Science for Business Applications 2022 CBS - Digi Welcome to the \u0026ldquo;Data Science for Business Applications\u0026rdquo; page. Throughout this course, you will aquire skills, knowledge, and capabilities to navigate and actively participate in the application of data science, machine learning, and artificial intelligence techniques in business. We hope you are excited.\nNote that this page rather than Canvas will represent the central hub for teaching material. At DSBA we believe in the power of open science and open education, and consequently we make all our material available outside password protected university systems.\nThe corresponding canvas course page can be found here, and will not be updated for other things than providing the live-lecture calendar.\n"},{"uri":"https://sds-aau.github.io/DSBA-2022/tags/","title":"Tags","tags":[],"description":"","content":""}]